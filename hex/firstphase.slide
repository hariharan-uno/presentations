Object Classification using HEX graphs

Hari haran
Neha Singh Chauhan

* Abstract
We study how to perform object classification in a principled way that exploits the rich structure of real world labels.
We base our project on a new model developed by Jia Deng, Nan Ding, et al., that allows encoding of flexible relations between labels.
We study *Hierarchy*and*Exclusion*(HEX)* graphs, a new formalism that captures semantic relations between any two labels applied to the same object: mutual exclusion, overlap and subsumption.


* Problem Statement
.image images/problem.png
.caption Given a feature vector, assign probabilities to each of a set of labels

* Current Approaches

* Multiclass Classification
predict one label from a set of mutually exclusive labels (e.g ILSVRC)

.image images/imagenet.png
.caption Large Scale Visual Recognition Challenge 

* Binary Classification
make decisions about each label independently (e.g PASCAL VOC Challenges)

.image images/pascalvoc.png
.caption Visual Object Classes Challenges

* Erraneous Assumptions

* 

Both the models do not accurately capture the semantic complexity of real world.

*Multiclass*classification* tasks typically assume a set of mutually exclusive labels. We can constrain the label set to be mutually exclusive but this assumption becomes increasingly impractical as we consider larger, more realistic label sets.

This is because the same object can often be described by multiple labels.

* 

An object classified as _husky_ is automatically a _dog_, meanwhile it may or may not be a _puppy_.

Making _husky_, _dog_, and _puppy_ mutually exclusive labels clearly violates real world semantics.

* 

*Independent*binary*classifiers*, on the other hand, ignore the constraints between labels and can thus handle overlapping labels. But this can lead to inconsistent predictions such as an object being both a dog and a cat, or a husky but not a dog.

In addition, discarding the label relations misses the opportunity to transfer knowledge during learning. 

* Example

In practical settings training images are not always annotated to the most specific labels,  many Internet images are simply labeled as _dog_ instead of _husky_ or _German_Shepherd_.

Intuitively, learning a good model for _dog_ should benefit learning breeds of dogs (and vice versa) but training independent binary classifiers will not help in transferring this potential knowledge.

* Goal

Our goal is to develop a new classification model that allows flexible encoding of relations based on prior knowledge, thus overcoming the limitations of the overly restrictive multiclass model and the overly relaxed independent binary classifiers

* Approach
To overcome the previous limitations,we take into account the semantic structure of the labels and model them as Hierarchy and Exclusion (HEX) graphs. We approach it in 2 parts:

- Formulating Hierarchy and Exclusion (HEX) graphs
- Utilizing a probabilistic classification model leveraging HEX graphs

* HEX Graphs
A new formalism allowing flexible specification of relations between labels applied to the same object: 

- mutual exclusion (e.g. an object cannot be dog and cat)
- overlapping (e.g. a husky may or may not be a puppy and vice versa)
- subsumption (e.g. all huskies are dogs).


* Example
.image images/hex_example.png
.caption HEX graphs capture relations between labels applied to the same object.

* Constraints
- Each label takes binary values.
Each edge then defines a constraint on values the two labels can take.

- A hierarchy edge (vi, vj) means an assignment of (vi, vj) = (0,1) is illegal.
- An exclusion edge (vi, vj) means an assignment of (vi, vj) = (1,1) is illegal.

These local constraints allow us to assign global values legally.

* Classification Model
A HEX graph encodes our prior knowledge about label relations. We can thus define a probabilistic classification model based on a HEX graph G = (V, Eh , Ee)

_x_ is the input
_y_ are labels
_f_ is the function with parameters _w_, that maps an input to a set of scores, one for each label. It can be a linear model or a Deep Neural Network.

* Distribution

.image images/jointdist.png
.caption The Joint distribution of the assignment is like a typical CRF but assigns probability to valid states


To compute the probability of a label, we marginalize all other labels. The scores _fi_(x;_w)_ can be thought of as raw classification scores (local evidence) for each label and our model can convert them into marginal probabilities.

* Properties of the model
- the probability of any illegal assignment is zero
- to compute the probability of a legal assignment, we take all labels with value 1, sum their scores, exponentiate, and then normalize
- the marginal label probabilities are always consistent with the label relations: probability of _dog_ is always bigger than that of _husky_ and probabilities of _dog_ and _cat_ cannot add to more than 1
- the model handles unknown categories gracefully by assuming an open world. 

* Implementation
We are using a feedforward neural network layer with a hidden layer size of 10.

* Training

During training, we are updating weights and bias using Scaled conjugate gradient backpropagation

* HEX graph CRF layer

We take the raw scores obtained from the feed forward neural network and use the probabilistic model of HEX graphs.
i.e. we take all labels with value 1, sum their scores, exponentiate, and then normalize.

.image images/jointdist.png
.caption The Joint distribution of the assignment is like a typical CRF but assigns probability to valid states

We obtain probabilistic values for each label in accordance.

* Future work

- Use a specialized loss function to improve accuracy.
- Implement exact inference
- Analyze the effects of sparsification and densification of the HEX graph.


* References

Deng, Jia, et al. "Large-Scale Object Classification using Label Relation Graphs." Computer Vision-ECCV 2014. Springer International Publishing, 2014. 48-64.

Lafferty, John, Andrew McCallum, and Fernando CN Pereira. "Conditional random fields: Probabilistic models for segmenting and labeling sequence data." (2001).

